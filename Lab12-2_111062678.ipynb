{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6ba8f956-6f69-409e-a7e1-c5a315523b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ad6cd194-e19f-4c33-b45f-445437681936",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./words_captcha/a3.png\n",
      "<start> i d s <end>\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "annotation_file = './words_captcha/spec_train_val.txt'\n",
    "IMAGE_DIR = './words_captcha/'\n",
    "\n",
    "img_name_list = []\n",
    "caption_list = []\n",
    "\n",
    "with open(annotation_file) as f:\n",
    "    for line in f:\n",
    "        image_name, caption = line.strip().split()\n",
    "        img_name_list.append(f'./words_captcha/{image_name}.png')\n",
    "        caption_list.append('<start> ' + ' '.join(caption) + ' <end>')\n",
    "\n",
    "test_img_name = set(glob.glob(f'./words_captcha/*.png')) - set(img_name_list)\n",
    "img_name_list += sorted(test_img_name)\n",
    "\n",
    "print(img_name_list[3])\n",
    "print(caption_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "632c2a2a-04db-4b3e-b60e-6c8061e8fdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  9 18 ...  6  3  0]\n",
      " [ 2 24 24 ...  3  0  0]\n",
      " [ 2  9  8 ... 13  3  0]\n",
      " ...\n",
      " [ 2 24  5 ...  3  0  0]\n",
      " [ 2 14  5 ...  4  3  0]\n",
      " [ 2  6  8 ... 23  3  0]]\n",
      "7\n",
      "<start> t i e d <end>\n",
      "[ 2  9  8  4 13  3  0]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='', filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(caption_list)\n",
    "cap_seqs = tokenizer.texts_to_sequences(caption_list)\n",
    "\n",
    "cap_seqs = tf.keras.preprocessing.sequence.pad_sequences(cap_seqs, padding='post')\n",
    "max_length = len(cap_seqs[0])\n",
    "\n",
    "print(cap_seqs)\n",
    "print(max_length)\n",
    "print(caption_list[2])\n",
    "print(cap_seqs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ee91a110-f06b-425d-9da1-efab06063ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train, img_name_valid = img_name_list[:100000], img_name_list[100000:120000]\n",
    "cap_seqs_train, cap_seqs_valid = cap_seqs[:100000], cap_seqs[100000:]\n",
    "img_name_test = img_name_list[120000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40e66ca6-ff44-408b-9dae-cd906e62a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (160, 300)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 5000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "EPOCHS = 10\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11129723-3c5c-41d5-b2b2-3e4500067893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(img_path, cap_seq):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = img / 255 * 2 - 1 #normalize image\n",
    "    return img, cap_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b7dfc782-e151-4b53-b67e-496a08c6b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((img_name_train, cap_seqs_train))\\\n",
    "                               .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .shuffle(BUFFER_SIZE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices((img_name_valid, cap_seqs_valid))\\\n",
    "                               .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "54fab9bb-013e-43d8-8d06-5f22abbc7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class conv_leaky_relu(layers.Layer):\n",
    "    def __init__(self, filters, size, stride):\n",
    "        super(conv_leaky_relu, self).__init__()\n",
    "        self.conv_2d = layers.Conv2D(filters, size, stride, padding='same')\n",
    "        self.batch_norm = layers.BatchNormalization()\n",
    "        self.leakey_relu = layers.LeakyReLU(0.1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv_2d(inputs)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.leakey_relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ad4ca339-7533-44d5-bd42-f858a71db283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, layers, Model\n",
    "\n",
    "inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "x = conv_leaky_relu(64, 7, 2)(inputs)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(192, 3, 1)(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(128, 1, 1)(x)\n",
    "x = conv_leaky_relu(256, 3, 1)(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = conv_leaky_relu(512, 1, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(512, 1, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "x = conv_leaky_relu(512, 1, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 2)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "outputs = conv_leaky_relu(1024, 3, 1)(x)\n",
    "\n",
    "feature_extractor = Model(inputs=inputs, outputs=outputs, name='YOLO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ed6a44f6-f2ff-4055-98ed-370d538af784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"YOLO\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 160, 300, 3)]     0         \n",
      "                                                                 \n",
      " conv_leaky_relu_48 (conv_le  (None, 80, 150, 64)      9728      \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 40, 75, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv_leaky_relu_49 (conv_le  (None, 40, 75, 192)      111552    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 20, 37, 192)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv_leaky_relu_50 (conv_le  (None, 20, 37, 128)      25216     \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_51 (conv_le  (None, 20, 37, 256)      296192    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_52 (conv_le  (None, 20, 37, 256)      66816     \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_53 (conv_le  (None, 20, 37, 512)      1182208   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 10, 18, 512)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv_leaky_relu_54 (conv_le  (None, 10, 18, 256)      132352    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_55 (conv_le  (None, 10, 18, 512)      1182208   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_56 (conv_le  (None, 10, 18, 256)      132352    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_57 (conv_le  (None, 10, 18, 512)      1182208   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_58 (conv_le  (None, 10, 18, 256)      132352    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_59 (conv_le  (None, 10, 18, 512)      1182208   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_60 (conv_le  (None, 10, 18, 256)      132352    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_61 (conv_le  (None, 10, 18, 512)      1182208   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_62 (conv_le  (None, 10, 18, 512)      264704    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_63 (conv_le  (None, 10, 18, 1024)     4723712   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 5, 9, 1024)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv_leaky_relu_64 (conv_le  (None, 5, 9, 512)        526848    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_65 (conv_le  (None, 5, 9, 1024)       4723712   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_66 (conv_le  (None, 5, 9, 512)        526848    \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_67 (conv_le  (None, 5, 9, 1024)       4723712   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_68 (conv_le  (None, 5, 9, 1024)       9442304   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_69 (conv_le  (None, 3, 5, 1024)       9442304   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_70 (conv_le  (None, 3, 5, 1024)       9442304   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      " conv_leaky_relu_71 (conv_le  (None, 3, 5, 1024)       9442304   \n",
      " aky_relu)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,208,704\n",
      "Trainable params: 60,182,336\n",
      "Non-trainable params: 26,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6a91f5d3-75b3-4535-91e0-18449045f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2dab3010-cb7d-4ff9-b61e-415b64e3d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b7b3484b-dda1-49d4-af7e-49199280e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "227c86b7-87d5-4177-8640-476283c2630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "67bf0111-2354-49a1-ad1b-b68d473b66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "617d432e-efcb-4ad1-ac9c-0f83c8157ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "checkpoint_path = './checkpoints/YOLO/'\n",
    "ckpt = tf.train.Checkpoint(feature_extractor=feature_extractor,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2251aa93-d1d3-4ba8-aab0-68962a4335fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "41b80507-8479-43aa-83d2-ce42cd24e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0dc3aaa5-1b67-420b-a365-660c39b8c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    \n",
    "    hidden = decoder.reset_state(batch_size=BATCH_SIZE)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        features = feature_extractor(img_tensor, True)\n",
    "        features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = feature_extractor.trainable_variables + encoder.trainable_variables + decoder.trainable_variables\n",
    "    \n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "62ca7af6-7d71-4bc8-a225-2c1550f8cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|█████████████████████████████████████████████████████████| 1000/1000 [08:06<00:00,  2.06it/s, loss=1.23]\n",
      "Epoch  2: 100%|████████████████████████████████████████████████████████| 1000/1000 [07:57<00:00,  2.09it/s, loss=0.157]\n",
      "Epoch  3: 100%|███████████████████████████████████████████████████████| 1000/1000 [07:57<00:00,  2.10it/s, loss=0.0491]\n",
      "Epoch  4: 100%|███████████████████████████████████████████████████████| 1000/1000 [08:03<00:00,  2.07it/s, loss=0.0311]\n",
      "Epoch  5: 100%|███████████████████████████████████████████████████████| 1000/1000 [07:58<00:00,  2.09it/s, loss=0.0207]\n",
      "Epoch  6: 100%|███████████████████████████████████████████████████████| 1000/1000 [08:00<00:00,  2.08it/s, loss=0.0172]\n",
      "Epoch  7: 100%|███████████████████████████████████████████████████████| 1000/1000 [07:56<00:00,  2.10it/s, loss=0.0141]\n",
      "Epoch  8: 100%|███████████████████████████████████████████████████████| 1000/1000 [07:57<00:00,  2.09it/s, loss=0.0116]\n",
      "Epoch  9: 100%|███████████████████████████████████████████████████████| 1000/1000 [07:52<00:00,  2.12it/s, loss=0.0117]\n",
      "Epoch 10: 100%|██████████████████████████████████████████████████████| 1000/1000 [08:01<00:00,  2.08it/s, loss=0.00784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 10 epoch 4829.953393936157 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "loss_plot = []\n",
    "start = time.time()\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    loss = 0\n",
    "    pbar = tqdm(dataset_train, total=num_steps, desc=f'Epoch {epoch + 1:2d}')\n",
    "    for (step, (img_tensor, target)) in enumerate(pbar):\n",
    "        loss += train_step(img_tensor, target)\n",
    "        pbar.set_postfix({'loss': loss.numpy() / (step + 1)})\n",
    "\n",
    "    loss_plot.append(loss / num_steps)\n",
    "    ckpt_manager.save()\n",
    "\n",
    "    # score = evaluate(dataset_valid)\n",
    "    # print(f'Validation accuracy: {score:.2f}')\n",
    "\n",
    "print('Time taken for {} epoch {} sec\\n'.format(EPOCHS - start_epoch, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "be5b6f7b-ea76-4891-9e6a-816ed15b8ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2ab0f036-6907-4a0d-858c-4e764d115720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1dbdee40b20>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(feature_extractor=feature_extractor,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt.restore('./checkpoints/YOLO/ckpt-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f19710e8-7cff-40b7-99b3-b4796a384944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "total_equal = 0\n",
    "total_loss = 0\n",
    "\n",
    "START = tokenizer.word_index['<start>']\n",
    "END = tokenizer.word_index['<end>']\n",
    "\n",
    "for (batch, (img_tensor, target)) in enumerate(dataset_valid):\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    features = feature_extractor(img_tensor,False)\n",
    "    features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "\n",
    "    # result = []\n",
    "    result = np.full((BATCH_SIZE, 1), START)\n",
    "\n",
    "    for i in range(1, target.shape[1]):\n",
    "\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "\n",
    "        loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "        result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "\n",
    "    target_array = target.numpy()\n",
    "\n",
    "    total_loss += (loss / int(target.shape[1]))\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        for j in range(max_length):\n",
    "            if result[i][j] == END and target_array[i][j] == END:\n",
    "                if (result[i][1:j] == target_array[i][1:j]).all():\n",
    "                    total_equal+=1\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c2b1b85c-a172-47e0-ab50-256189133922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Accuracy 0.922, Validation Loss 0.073\n"
     ]
    }
   ],
   "source": [
    "valid_acu = round( float(total_equal ) / (( batch + 1 ) * BATCH_SIZE ), 3)\n",
    "valid_loss = round(float(total_loss / (batch + 1)), 3)\n",
    "print (f'Valid Accuracy {valid_acu}, Validation Loss {valid_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "98def3cb-fb38-4343-b123-1a2d247a25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_test(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = img / 255 * 2 - 1\n",
    "    return img, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f32dd7b4-7f08-403d-8dc6-569bc4b47171",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices((img_name_test))\\\n",
    "                              .map(map_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                              .batch(100)\\\n",
    "                              .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bda76ee9-5171-491f-872c-0446923edc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 160, 300, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "74d08985-d935-4824-8789-f3c9c4cefd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_tensor):\n",
    "    batch_size = img_tensor.shape[0]\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "    features = feature_extractor(img_tensor)\n",
    "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    result = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    for _ in range(max_length):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions, axis=1).numpy()\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        result = tf.concat([result, predicted_id.reshape((batch_size, 1))], axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e57fd80a-d049-45d9-a650-a98f5fab374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "output = './Lab12-2_111062678.txt'\n",
    "with open(output, 'w') as fout:\n",
    "    for step, (img_tensor, img_path) in enumerate(dataset_test):\n",
    "        segs = predict(img_tensor).numpy();\n",
    "        pred_list = []\n",
    "        for seq in segs:\n",
    "            result = ''\n",
    "            for s in seq[1:]:\n",
    "                if s == tokenizer.word_index['<end>']:\n",
    "                    break\n",
    "                result += tokenizer.index_word[s]\n",
    "            pred_list.append(result)\n",
    "        \n",
    "        for path, pred in zip(img_path, pred_list):\n",
    "            path = path.numpy().decode('utf-8')\n",
    "            name = re.search('(a[0-9]+)', path).group(1)\n",
    "            fout.write(f'{name} {pred}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2660f-87e4-42a1-839f-534522b16173",
   "metadata": {},
   "source": [
    "### Report\n",
    "架構和Image Captioning一樣是透過CNN做feature extraction，再接上CNN encoder和RNN decoder，在lab中的Image Captioning 主要是focus在word，但這裡主要是透過圖片預測各個單字，因此這邊會將每個單字賦予一個編號。此處的feature extraction的架構是遵循YOLO而設計的，encoder和decoder是遵照lab上的code，在訓練過程中，我設置了10個epochs，最終利用第十個epoch的weight得到了0.922的Valid Accuracy。之後會想再試試看能不能透過置換feature extraction的CNN架構來更進一步的提升Validation Accuracy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f3966-9c65-4c64-9b86-d0b2bfb6e114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
